{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 23259\n",
    "num_feats = 300\n",
    "num_labels = 4\n",
    "feat_data, ori_adj_lists, pos_triplets = load_dblp()\n",
    "features = nn.Embedding(num_nodes, num_feats)\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "adj_lists = copy.deepcopy(ori_adj_lists)\n",
    "for i in test_idx:\n",
    "    if labels[i]==1:\n",
    "        sub, rel, obj = triplets[i]\n",
    "        adj_lists[rel][sub].remove(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[333370, 53544, 129246, 129246, 11413, 11413, 11701, 11701, 25735, 25735]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(np.where(pos_triplets[:,1]==rel_idx)[0]) for rel_idx in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_sampling(pos_triplets):\n",
    "    # according to the number of relations to choose a relation\n",
    "    rel_cnt = np.array([len(np.where(pos_triplets[:,1]==\n",
    "                           rel_idx)[0]) for rel_idx in range(10)])\n",
    "    rel_prob = rel_cnt / np.sum(rel_cnt)\n",
    "    return np.random.choice(np.arange(10), p=rel_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_triplets_smapling(rel_idx, triplets, train_idx):\n",
    "    # sample triplets of batch 256 in this relation\n",
    "    rel_train_idx = train_idx[triplets[train_idx,1]==rel_idx]\n",
    "    return np.array(rel_train_idx[:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rel = 10\n",
    "def cnt_node_rel(triplets):\n",
    "    node_rel_table = np.zeros((num_nodes, num_rel))\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_rel):\n",
    "            node_rel_table[i,j] = len(np.intersect1d(\n",
    "                np.where(triplets[:,0]==i),np.where(triplets[:,1]==j)))\n",
    "    return node_rel_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-10b94362960b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnt_node_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_triplets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-365339cf80f5>\u001b[0m in \u001b[0;36mcnt_node_rel\u001b[0;34m(triplets)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             node_rel_table[i,j] = len(np.intersect1d(\n\u001b[0;32m----> 7\u001b[0;31m                 np.where(triplets[:,0]==i),np.where(triplets[:,1]==j)))\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnode_rel_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36mintersect1d\u001b[0;34m(ar1, ar2, assume_unique)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;31m# Might be faster than unique( intersect1d( ar1, ar2 ) )?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mar1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mar2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid axis kwarg specified for unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnt_node_rel(pos_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      1,      12,      14, ..., 8121447, 8121467, 8121471])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(np.where(triplets[:,0]==1),np.where(triplets[:,1]==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 2]), 2921)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets[12], len(np.intersect1d(np.where(triplets[:,0]==1),np.where(triplets[:,1]==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True, False, False, ...,  True,  True,  True]),\n",
       " array([ 491351, 4606956, 2151503, ..., 4724836, 4493213, 6448727]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets[train_idx,1]==0, train_idx[triplets[train_idx,1]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6539312"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2932948, 6539312)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx[triplets[train_idx,1]==0]),len(triplets[train_idx,1]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_sampling(pos_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6539312, 1634832)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx), len(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(triplets, num_nodes):\n",
    "#     print(len(triplets))\n",
    "    labels = np.ones((len(triplets)*(10+1),1))\n",
    "    labels[len(triplets):] = 0\n",
    "    # random.shuffle(labels)\n",
    "    # triplets = np.array(triplets)\n",
    "    neg_num = 10 * len(triplets)\n",
    "    neg_triplets = np.tile(triplets, (10, 1))\n",
    "    neg_nodes = np.random.randint(num_nodes, size=neg_num)\n",
    "    neg_choice = np.random.uniform(size=neg_num)\n",
    "    neg_sub_idx, neg_obj_idx = neg_choice>0.5, neg_choice<=0.5\n",
    "    neg_triplets[neg_sub_idx,0] = neg_nodes[neg_sub_idx] \n",
    "    neg_triplets[neg_obj_idx,2] = neg_nodes[neg_obj_idx]\n",
    "    return np.concatenate((triplets, neg_triplets),axis=0), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets, labels = negative_sampling(pos_triplets, num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6539312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_idx = np.random.permutation(len(triplets))\n",
    "train_idx = random_idx[:len(triplets)//10*8]\n",
    "test_idx = random_idx[len(triplets)//10*8:]\n",
    "len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128])\n",
      "torch.Size([10, 300, 128])\n",
      "torch.Size([10, 128, 128])\n",
      "torch.Size([23259, 300])\n"
     ]
    }
   ],
   "source": [
    "for p in list(rgcn_graphsage.parameters()):\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.913046875\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    batch_test_output = rgcn_graphsage.forward(triplets[test_idx[i*256:(i+1)*256]])\n",
    "    if i == 0:\n",
    "        test_output = batch_test_output\n",
    "    else:\n",
    "        test_output = torch.cat((test_output, batch_test_output))\n",
    "test_output[test_output>0.5]=1\n",
    "test_output[test_output<=0.5]=0\n",
    "print(\"Test Accuracy:\", sklearn.metrics.accuracy_score(labels[test_idx[:50*256]],\n",
    "                                                              test_output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 344]) torch.Size([344, 300])\n",
      "torch.Size([10, 20]) torch.Size([200, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0067, 0.0000, 0.0000, 0.0190, 0.0110, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0244, 0.0000, 0.0000, 0.0000, 0.0092, 0.0000, 0.0000, 0.0000,\n",
       "         0.0228, 0.0000, 0.0136, 0.0052, 0.0043, 0.0000, 0.0000, 0.0107, 0.0000,\n",
       "         0.0137, 0.0000, 0.0091, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0043,\n",
       "         0.0136, 0.0000, 0.0000, 0.0044, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0132, 0.0057, 0.0000, 0.0188, 0.0000, 0.0000, 0.0004, 0.0186, 0.0195,\n",
       "         0.0029, 0.0035, 0.0254, 0.0000, 0.0194, 0.0122, 0.0057, 0.0043, 0.0129,\n",
       "         0.0126, 0.0231, 0.0000, 0.0111, 0.0000, 0.0000, 0.0000, 0.0138, 0.0171,\n",
       "         0.0207, 0.0000, 0.0108, 0.0000, 0.0046, 0.0032, 0.0000, 0.0000, 0.0099,\n",
       "         0.0167, 0.0080, 0.0246, 0.0000, 0.0000, 0.0223, 0.0221, 0.0100, 0.0000,\n",
       "         0.0068, 0.0019, 0.0042, 0.0000, 0.0188, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0054, 0.0070, 0.0190, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0005, 0.0147, 0.0000, 0.0040, 0.0025, 0.0004, 0.0021, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0104, 0.0000, 0.0000, 0.0000, 0.0000, 0.0206,\n",
       "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding\n",
    "rgcn_graphsage.encoder.forward([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10350, 6905]) torch.Size([6905, 300])\n",
      "torch.Size([1000, 1035]) torch.Size([10350, 128])\n",
      "torch.Size([10700, 6987]) torch.Size([6987, 300])\n",
      "torch.Size([1000, 1070]) torch.Size([10700, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6935, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rgcn_graphsage.loss(batch_triplets, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features, num_feats, h_dim, num_rel, adj_lists):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.aggregator_1 = Aggregator(adj_lists, num_feats)\n",
    "#         self.feat_dim = features.shape[1]\n",
    "        self.features = features\n",
    "        self.weight_1 = nn.Parameter(\n",
    "            torch.FloatTensor(num_rel, num_feats, h_dim))\n",
    "        nn.init.xavier_uniform_(self.weight_1)\n",
    "        \n",
    "        self.aggregator_2 = Aggregator(adj_lists, h_dim, second_cut=True)\n",
    "        self.weight_2 = nn.Parameter(\n",
    "            torch.FloatTensor(num_rel, h_dim, h_dim))\n",
    "        nn.init.xavier_uniform_(self.weight_2)\n",
    "        \n",
    "    def encode(self, nodes):\n",
    "        aggr_feats, rel_idxs = self.aggregator_1.forward(nodes, self.features)\n",
    "        x = F.relu(torch.bmm(aggr_feats, torch.index_select(self.weight_1, dim=0, index=rel_idxs)))\n",
    "#         print('first cut x mean shape', x.mean(0).shape)\n",
    "        return x.mean(0)\n",
    "        \n",
    "    def forward(self, nodes):\n",
    "#         aggre_feat = self.aggregator_1.forward(nodes, self.features)\n",
    "#         print(aggre_feat.type(),aggre_feat)\n",
    "#         x = F.relu(torch.mm(self.aggregator_1.forward(nodes, self.features)\n",
    "#                             , self.weight_1))\n",
    "        aggr_feats, rel_idxs = self.aggregator_2.forward(nodes, self.encode)   \n",
    "        x = F.relu(torch.bmm(aggr_feats, torch.index_select(self.weight_2, dim=0, index=rel_idxs)))\n",
    "#         print('torch.index_select(self.weight_2, dim=0, rel_idx)', \n",
    "#               (torch.index_select(self.weight_2, dim=0, index=rel_idxs)).shape)\n",
    "#         print('x.shape', x.shape)\n",
    "        x = x.mean(0)\n",
    "#         print('encode mean 0 x.shape', x.shape)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_neighs = [[adj_lists[rel_idx][int(node)] for node in [0,1,2,3,4,5]]\\\n",
    "                        for rel_idx in [0,1,2,3,4,5,6,7,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgcn_graphsage.encoder.weight_2.requires_grad, rgcn_graphsage.weight_relation.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "1 5 tensor(0.6933, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "2 4 tensor(2.9336, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "3 4 tensor(1.7260, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "4 1 tensor(0.6951, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "5 4 tensor(0.6919, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "6 5 tensor(0.6934, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "7 8 tensor(0.6869, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "8 3 tensor(0.6363, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "9 8 tensor(4.5786, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "10 8 tensor(0.8641, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "11 0 tensor(0.6024, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "12 5 tensor(1.0719, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "13 4 tensor(0.6966, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "14 1 tensor(0.6876, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "15 2 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "16 4 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "17 4 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "18 2 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "19 1 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "20 7 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "21 5 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "22 5 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "23 6 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "24 7 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "25 0 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "26 3 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "27 5 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "28 6 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "29 3 tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "rgcn_graphsage = RGCN_GraphSAGE(features, num_feats, 128, 10, adj_lists)\n",
    "optimizer = torch.optim.Adam(rgcn_graphsage.parameters(), lr=0.3)\n",
    "for i in range(30):\n",
    "#     rel_idx = rel_sampling(pos_triplets)\n",
    "    rel_idx = np.random.randint(10)\n",
    "    bacth_train_idx = rel_triplets_smapling(rel_idx, triplets, train_idx)\n",
    "#     bacth_train_idx = np.array(train_idx[:256])\n",
    "    random.shuffle(train_idx)\n",
    "    \n",
    "    batch_triplets = triplets[bacth_train_idx]\n",
    "    batch_labels = torch.FloatTensor(labels[bacth_train_idx])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = rgcn_graphsage.loss(batch_triplets, batch_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i, rel_idx, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(np.array([0.02,0.3,0.1]))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(nn.Module):\n",
    "    def __init__(self, adj_lists, dim, second_cut=False):\n",
    "        super(Aggregator, self).__init__()\n",
    "#         self.features = features\n",
    "        self.second_cut = second_cut\n",
    "        self.adj_lists = adj_lists\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, nodes, features, num_sample=10):\n",
    "        \"\"\"\n",
    "        nodes --- list of nodes in a batch\n",
    "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
    "        num_sample --- number of neighbors to sample. No sampling if None.\n",
    "        \"\"\"\n",
    "        self.features=features\n",
    "        \n",
    "        # outgoing\n",
    "#         print('nodes',nodes)\n",
    "#         print('nodes[-1]',nodes[-1])\n",
    "#         for adj in self.adj_lists:\n",
    "#             print(len(adj[nodes[-1]]))\n",
    "        nodes_rels_cnt = np.array([[len(adj_list[node]) for node in nodes]\\\n",
    "                  for adj_list in self.adj_lists])\n",
    "#         print('print(nodes_rels_cnt)', nodes_rels_cnt)\n",
    "        nodes_rels_cnt = nodes_rels_cnt.sum(1)\n",
    "#         print('nodes_rels_cnt', nodes_rels_cnt)\n",
    "#         print('np.sum(nodes_rels_cnt)', np.sum(nodes_rels_cnt))\n",
    "        rel_prob = nodes_rels_cnt / np.sum(nodes_rels_cnt)\n",
    "        # sample 3 relations\n",
    "#         print('rel_prob',rel_prob)\n",
    "        possible_num_rels = np.sum(rel_prob>=0.1)\n",
    "        sort_rels_idxs = np.argsort(rel_prob)[::-1].copy()\n",
    "        rel_idxs = sort_rels_idxs[:possible_num_rels]\n",
    "#         rel_idxs = np.random.choice(10, p=rel_prob, size=3)\n",
    "#         print('rel_idxs',rel_idxs)\n",
    "        all_rels_neighs = [[self.adj_lists[rel_idx][node] for node in nodes]\\\n",
    "                        for rel_idx in rel_idxs]\n",
    "        \n",
    "        # totally sample 10 neighbors, came from 3 relations\n",
    "#         rel_sample_neighs = []\n",
    "        aggr_rel_feats = torch.Tensor(len(rel_idxs), len(nodes), self.dim)\n",
    "        for r, rel_idx in enumerate(rel_idxs):\n",
    "            rel_neighs = all_rels_neighs[r]\n",
    "#             print('len(rel_neighs)',len(rel_neighs))\n",
    "#             print('len(rel_prob)', len(rel_prob), rel_prob)\n",
    "#             print('rel_idx',rel_idx)\n",
    "            \n",
    "            rel_sample_neighs = [set(random.sample(rel_neighs[i], int(10*rel_prob[rel_idx])))\\\n",
    "                                 if len(rel_neighs[i]) >= int(10*rel_prob[rel_idx]) else rel_neighs[i]\\\n",
    "                                 for i in range(len(nodes))]\n",
    "#             print('rel_sample_neighs',rel_sample_neighs)\n",
    "#             print('rel_sample_neighs',rel_sample_neighs)\n",
    "            rel_sample_neighs_list = list(set.union(*rel_sample_neighs))\n",
    "            # n: actual node index\n",
    "            rel_sample_neighs_dict = {neigh:i for i,neigh in enumerate(rel_sample_neighs_list)}\n",
    "            mask = torch.zeros(len(nodes), len(rel_sample_neighs_list))\n",
    "            \n",
    "            row_idxs = [i for i in range(len(nodes)) for _ in rel_sample_neighs[i]]\n",
    "            col_idxs = [rel_sample_neighs_dict[neigh] for node_rel_sample_neighs in rel_sample_neighs\\\n",
    "                       for neigh in node_rel_sample_neighs]\n",
    "#             print('row_idxs',row_idxs)\n",
    "#             print('col_idxs',col_idxs)\n",
    "            \n",
    "            mask[row_idxs, col_idxs] = 1\n",
    "#             print('len(rel_sample_neighs_list)', len(rel_sample_neighs_list))\n",
    "            \n",
    "            # to deal with np.nan\n",
    "            num_neigh = mask.sum(1, keepdim=True)\n",
    "            thre_zeros = torch.zeros_like(mask)\n",
    "            # num_neigh = mask.sum()/(mask.shape[0])\n",
    "            # mask = mask.div(num_neigh)\n",
    "            mask = torch.where(num_neigh != 0, mask.div(num_neigh), thre_zeros)\n",
    "            \n",
    "            if self.second_cut:\n",
    "                embed_matrix = self.features(rel_sample_neighs_list)\n",
    "            else:\n",
    "                embed_matrix = self.features(torch.LongTensor(rel_sample_neighs_list))\n",
    "            \n",
    "#             print('mask.shape, embed_matrix.shape',mask.shape, embed_matrix.shape)\n",
    "            aggr_rel_feats[r,:,:] = torch.mm(mask, embed_matrix)\n",
    "#             print('aggr_rel_feats.requires_grad',aggr_rel_feats.requires_grad)\n",
    "#             print('embed_matrix.requires_grad',embed_matrix.requires_grad)\n",
    "        \n",
    "#         for i in len(rel_idxs):\n",
    "#             mask = torch.zeros(len(nodes), len(unique_nodes))\n",
    "#         multi_to_neighs = [[single_adj_lists[int(node)] for node in nodes]\\\n",
    "#                      for single_adj_lists in self.adj_lists]\n",
    "#         _set = set\n",
    "#         if not num_sample is None:\n",
    "#             _sample = random.sample\n",
    "#             multi_samp_neighs = [[_set(_sample(to_neigh, \n",
    "#                             num_sample,\n",
    "#                             )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "#                             for to_neighs in multi_to_neighs]\n",
    "#         else:\n",
    "#             multi_samp_neighs = to_neighs\n",
    "\n",
    "        # print(multi_samp_neighs)\n",
    "\n",
    "#         if self.gcn:\n",
    "#             samp_neighs = [samp_neigh + set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "#         multi_unique_nodes_list = [set.union(*samp_neighs) for samp_neighs in rel_sample_neighs]\n",
    "#         unique_nodes_list = list(set.union(*multi_unique_nodes_list))\n",
    "#         unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
    "#         mask = torch.zeros(len(rel_idxs), len(nodes), len(unique_nodes)) # no matter what relationship between them\n",
    "#         column_indices = [unique_nodes[n] for samp_neighs in rel_sample_neighs for samp_neigh in samp_neighs for n in samp_neigh]   \n",
    "#         row_indices = [i for r,samp_neighs in enumerate(rel_sample_neighs) for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "# #         rel_indices = [r for r,samp_neighs in enumerate(multi_samp_neighs) for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "#         mask[rel_indices, row_indices, column_indices] = 1\n",
    "        # no stack\n",
    "#         mask = mask.reshape((-1, len(unique_nodes))) # for stack adjcent matrixs for different relations\n",
    "#         if self.cuda:\n",
    "#             mask = mask.cuda()\n",
    "        # todo: normalize\n",
    "        # no stack\n",
    "#         num_neigh = mask.sum(2, keepdim=True)\n",
    "#         thre_zeros = torch.zeros_like(mask)\n",
    "#         # num_neigh = mask.sum()/(mask.shape[0])\n",
    "#         # mask = mask.div(num_neigh)\n",
    "#         mask = torch.where(num_neigh != 0, mask.div(num_neigh), thre_zeros)\n",
    "#         if self.cuda:\n",
    "#             embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "#         else:\n",
    "#         print('self.features type', type(self.features))\n",
    "#         embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "\n",
    "#         print(mask.shape, embed_matrix.shape)\n",
    "#         if self.second_cut:\n",
    "#             num_rels = len(multi_samp_neighs)\n",
    "#             num_rels, num_multi_cur_nodes, num_next_nodes = mask.shape\n",
    "#             print('mask reshape,', num_rels, num_multi_cur_nodes//num_rels, num_next_nodes)\n",
    "            # no stack\n",
    "#             mask = mask.reshape((num_rels, num_multi_cur_nodes//num_rels, num_next_nodes))\n",
    "#             num_multi_next_nodes, num_emb_dim = embed_matrix.shape\n",
    "#             embed_matrix = embed_matrix.reshape((num_rels, num_multi_next_nodes//num_rels, num_emb_dim))\n",
    "#             to_feats = torch.bmm(mask, embed_matrix)\n",
    "#             to_feats = torch.zeros(num_rels, num_multi_cur_nodes//num_rels, num_emb_dim)\n",
    "#             for i in range(num_rels):\n",
    "                # print(i, \"mask[i]\", mask[i], \"embed_matrix[i]\", embed_matrix[i])\n",
    "#                 to_feats[i] = mask[i].mm(embed_matrix[i])\n",
    "#             print('second cut : to_feats shape', to_feats.shape)\n",
    "#             import numpy as np\n",
    "#             print(np.where(np.isnan(to_feats.detach().numpy())==False))\n",
    "#             to_feats = to_feats.sum(0)\n",
    "#             to_feats = to_feats.reshape((num_multi_cur_nodes, num_emb_dim))\n",
    "#         else:\n",
    "#             # broadcast\n",
    "#             print('first cut: mask shape and embed_matrix shape', mask.shape, embed_matrix.shape)\n",
    "#             to_feats = torch.matmul(mask, embed_matrix)\n",
    "#             print('first cut: to_feats shape', to_feats.shape)\n",
    "#         print('rel_idxs',rel_idxs, 'torch.from_numpy', torch.from_numpy(rel_idxs))\n",
    "        return aggr_rel_feats, torch.from_numpy(rel_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN_GraphSAGE(nn.Module):\n",
    "    def __init__(self, features, num_feats, h_dim, num_rel, adj_lists):\n",
    "        super(RGCN_GraphSAGE, self).__init__()\n",
    "        self.encoder = Encoder(features, num_feats, h_dim, num_rel, adj_lists)\n",
    "        self.weight_relation = nn.Parameter(\n",
    "            torch.FloatTensor(num_rel, h_dim))\n",
    "        nn.init.xavier_uniform_(self.weight_relation)\n",
    "        \n",
    "    def forward(self, triplets):\n",
    "#         print(triplets[:,0])\n",
    "        sub = self.encoder.forward(triplets[:,0])\n",
    "        obj = self.encoder.forward(triplets[:,2])\n",
    "        rel = self.weight_relation[triplets[:,1]]\n",
    "#         print('sub obj rel shape', sub.shape, obj.shape, rel.shape)\n",
    "        return torch.sum(sub*obj*rel, dim=1, keepdim=True)\n",
    "    \n",
    "    def loss(self, triplets, labels):\n",
    "        score = self.forward(triplets)\n",
    "#         print('score.type {}, labels.type {}'.format(score.dtype, labels.dtype))\n",
    "#         print('score.shape {}, labels.shape {}'.format(score.shape, labels.shape))\n",
    "        return F.binary_cross_entropy_with_logits(score, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dblp():\n",
    "    num_nodes = 23260-1\n",
    "    num_feats = 300\n",
    "    num_rels = 10\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "#     labels = np.empty((num_nodes,1), dtype=np.int64)\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "\n",
    "    # feature\n",
    "    with open(\"dblp/sub.node.dat\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            feat_data[i,:] = list(map(float, info[-1].split(',')))\n",
    "#             labels[i] = float(info[-2])\n",
    "            name = ' '.join(info[:-2])\n",
    "            node_map[name] = i\n",
    "            # print(labels[i])\n",
    "            # if not info[-1] in label_map:\n",
    "                # label_map[info[-1]] = len(label_map)\n",
    "            # labels[i] = label_map[info[-1]]\n",
    "\n",
    "    # adjcent matrixs for each relation\n",
    "    adj_lists = [defaultdict(set) for i in range(num_rels)]\n",
    "    triplets = []\n",
    "#     labels = []\n",
    "    with open(\"dblp/sub.link.dat\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = map(int, line.strip().split())\n",
    "            sub, obj, rel, _ = info\n",
    "            triplets.append([sub, rel, obj])\n",
    "#             labels.append(1)\n",
    "            adj_lists[rel][sub].add(obj)\n",
    "            \n",
    "\n",
    "    # for debug puepose, print the dims\n",
    "    # print(feat_data.shape, labels.shape)\n",
    "    # for i in range(num_rels):\n",
    "    #     print(i, len(adj_lists[i]))\n",
    "    #     if i != 1:\n",
    "    #         continue\n",
    "    #     for key in adj_lists[i]:\n",
    "    #         print(key, len(adj_lists[i][key]))\n",
    "\n",
    "    return feat_data, adj_lists, np.array(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
